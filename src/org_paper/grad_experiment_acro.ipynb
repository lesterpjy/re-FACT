{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#notebook have been run on colab with original papers' data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r /content/drive/MyDrive/Uva/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --force-reinstall --no-deps  git+https://github.com/neelnanda-io/TransformerLens/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import string\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import utils, patching\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from string import ascii_uppercase\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from sentence_transformers.util import semantic_search, dot_score, normalize_embeddings\n",
    "from transformer_lens import HookedTransformer\n",
    "from src.org_paper.adv_gen import *\n",
    "\n",
    "torch.manual_seed(42);\n",
    "\n",
    "from src.utils.plotly_utils import imshow, line, scatter\n",
    "\n",
    "data_path = \"/content/drive/MyDrive/Uva/data/\"\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial sample generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# tokens of capital letters\n",
    "cap_tokens = model.to_tokens([x for x in ascii_uppercase], prepend_bos=False)[:, 0]\n",
    "cap_tokens_space = model.to_tokens([\" \" + str(x) for x in ascii_uppercase], prepend_bos=False)[:, 0]\n",
    "# as we are taking a subset of the vocabulary, we also enumerate them in order\n",
    "# (e.g 'A' is the token 32, but is enumerated as 0 on the subspace, etc.)\n",
    "idx_to_token = {k:v.item() for k, v in enumerate(cap_tokens)}\n",
    "token_to_idx = {v.item():k for k, v in enumerate(cap_tokens)}\n",
    "space_to_no_space = {k.item():v.item() for k,v in zip(cap_tokens_space, cap_tokens)}\n",
    "no_space_to_space = {k.item():v.item() for k,v in zip(cap_tokens, cap_tokens_space)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# indices of the token containing the first/second/third capital letters\n",
    "indices_letters = [2, 3, 4]\n",
    "# same for the acronym letter -1 (this is where the corresponding logit is stored)\n",
    "indices_logits = [5, 6, 7]\n",
    "# letter that we want to modify: 0, 1 or 2\n",
    "letter = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with open(f\"{data_path}/acronyms.txt\", \"r\") as f:\n",
    "   prompts, acronyms = list(zip(*[line.split(\", \") for line in f.read().splitlines()]))\n",
    "\n",
    "# take a subset of the dataset (we do this because VRAM limitations)\n",
    "n_samples = 1000\n",
    "\n",
    "prompts, acronyms = list(map(list, zip(*random.choices(list(zip(prompts, acronyms)), k=n_samples))))\n",
    "\n",
    "tokens = model.to_tokens(prompts)\n",
    "# ground truth: third letter of the acronym (tokens)\n",
    "y = model.to_tokens([x[letter] for x in acronyms], prepend_bos=False).squeeze()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(tokens)[:, indices_logits[letter]].argmax(dim=-1)\n",
    "# discard already misclassified samples\n",
    "tokens = tokens[y_pred == y]\n",
    "y = y[y_pred == y]\n",
    "# reupdate n_samples\n",
    "n_samples = tokens.shape[0]\n",
    "\n",
    "y_idx = y.cpu().apply_(token_to_idx.get).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "full_masked_maxs = []\n",
    "full_unmasked_maxs = []\n",
    "\n",
    "full_masked_means = []\n",
    "full_unmasked_means = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for seed in [42,2,3]:\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    # randomly sample batch_size initial samples\n",
    "    idx = torch.randint(0, n_samples, (batch_size,))\n",
    "    sample_tokens = tokens[idx]\n",
    "    sample_y = y[idx]\n",
    "    sample_y_idx = y_idx[idx]\n",
    "    sample_embeddings = model.W_E[sample_tokens].clone().detach()\n",
    "    sample_embeddings.requires_grad = True\n",
    "\n",
    "    embedding_matrix = model.W_E\n",
    "\n",
    "    # now the vocabulary of optimization is every single token word possible\n",
    "    with open(f\"{data_path}/words_dict.pkl\", 'rb') as f:\n",
    "        words_dict = pickle.load(f)\n",
    "    word_list = [v for _, v in words_dict.items()]\n",
    "    word_list = [x for xs in word_list for x in xs]\n",
    "    vocab = model.to_tokens(word_list, prepend_bos=False)[:, 0]\n",
    "\n",
    "    mask = torch.zeros(sample_tokens.shape[-1], dtype=torch.bool).cuda()\n",
    "    mask[indices_letters[letter]] = 1\n",
    "\n",
    "    lr=1e-1\n",
    "    weight_decay=1e-1\n",
    "    margin = 4\n",
    "    loss_fn = AdvMarginLoss(margin=margin)\n",
    "\n",
    "    gradients = []\n",
    "\n",
    "    input_optimizer = torch.optim.AdamW([sample_embeddings], lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # we will collect the adversarial samples: samples that are incorrectly classified by the model\n",
    "    adv_samples = []\n",
    "    # we also collect the original sample associated to each adversarial sample\n",
    "    original_samples = []\n",
    "    losses = []\n",
    "    # OPTIMIZE\n",
    "    for _ in tqdm(range(200), disable=False):\n",
    "\n",
    "        # Project the embeddings\n",
    "        projected_tokens, projected_embeddings = project_embeddings(sample_tokens,sample_embeddings, embedding_matrix, vocab, mask)\n",
    "        # BRUH this is causing a high bottleneck. Optimize when everything works right\n",
    "        sample_y_idx = torch.tensor([token_to_idx[model.to_tokens(model.to_string(x[indices_letters[letter]])[1], prepend_bos=False).item()] for x in projected_tokens], dtype=torch.long).cuda()\n",
    "\n",
    "        tmp_embeddings = sample_embeddings.detach().clone()\n",
    "        tmp_embeddings.data = projected_embeddings.data\n",
    "        tmp_embeddings.requires_grad = True\n",
    "\n",
    "        # Take the logits of the subspace\n",
    "        logits_vocab = model.forward(tmp_embeddings + model.pos_embed(projected_tokens), start_at_layer=0)[:, indices_logits[letter], cap_tokens]\n",
    "\n",
    "        loss = loss_fn(logits_vocab, sample_y_idx, average=True)\n",
    "\n",
    "        sample_embeddings.grad, = torch.autograd.grad(loss, [tmp_embeddings])\n",
    "        # set the gradient of elements outside the mask to zero\n",
    "        gradients.append(sample_embeddings.grad)\n",
    "\n",
    "        sample_embeddings.grad = torch.where(mask[None, ..., None], sample_embeddings.grad, 0.)\n",
    "        input_optimizer.step()\n",
    "        input_optimizer.zero_grad()\n",
    "        #print(loss.item())\n",
    "        #print(model.to_string(projected_tokens))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Re-project the embeddings\n",
    "            projected_tokens, projected_embeddings = project_embeddings(sample_tokens,sample_embeddings, embedding_matrix, vocab, mask)\n",
    "            sample_y_idx = torch.tensor([token_to_idx[model.to_tokens(model.to_string(x[indices_letters[letter]])[1], prepend_bos=False).item()] for x in projected_tokens], dtype=torch.long).cuda()\n",
    "            # check if there are adversarial samples\n",
    "            # Take the logits of the subspace\n",
    "            logits_vocab = model.forward(projected_embeddings + model.pos_embed(projected_tokens), start_at_layer=0)[:, indices_logits[letter], cap_tokens]\n",
    "\n",
    "            loss_i = loss_fn(logits_vocab, sample_y_idx, average=False)\n",
    "            adv_samples.append(projected_tokens[loss_i < margin]) # a loss lower than margin implies that the sample is incorrectly classified\n",
    "            original_samples.append(sample_tokens[loss_i < margin])\n",
    "\n",
    "    adv_samples = torch.cat(adv_samples, dim=0)\n",
    "    original_samples = torch.cat(original_samples, dim=0)\n",
    "    adv_samples, inverse_indices = torch.unique(adv_samples, sorted=False, dim=0, return_inverse=True)\n",
    "    original_samples_unique = torch.zeros_like(adv_samples)\n",
    "    original_samples_unique[inverse_indices] = original_samples[torch.arange(0, original_samples.shape[0])]\n",
    "    original_samples = original_samples_unique\n",
    "\n",
    "    masked_maxs, unmasked_maxs, masked_means, unmasked_means = grad_stat_org(mask, gradients)\n",
    "    \n",
    "    full_masked_maxs.append(masked_maxs)\n",
    "    full_unmasked_maxs.append(unmasked_maxs)\n",
    "\n",
    "    full_masked_means.append(masked_means)\n",
    "    full_unmasked_means.append(unmasked_means)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "lists_to_save = [full_masked_maxs,full_unmasked_maxs,full_masked_means,full_unmasked_means]\n",
    "names = ['full_masked_maxs','full_unmasked_maxs','full_masked_means','full_unmasked_means']\n",
    "\n",
    "for name, list_to_save in zip(names,lists_to_save):\n",
    "  with open(f'work/saved/acronym/{name}.pickle', 'wb') as handle:\n",
    "      pickle.dump(list_to_save, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with open('work/saved/acronym/gradients_seed_3_acronym.pickle', 'wb') as handle:\n",
    "    pickle.dump(gradients, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "full_masked_mean = np.array(full_masked_means)\n",
    "full_unmasked_mean = np.array(full_unmasked_means)\n",
    "\n",
    "# Compute mean and standard deviation over trials (axis 0)\n",
    "masked_mean = np.mean(full_masked_mean, axis=0)\n",
    "masked_std = np.std(full_masked_mean, axis=0)\n",
    "\n",
    "unmasked_mean = np.mean(full_unmasked_mean, axis=0)\n",
    "unmasked_std = np.std(full_unmasked_mean, axis=0)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Masked plot with mean and std\n",
    "plt.plot(range(full_masked_mean.shape[1]), masked_mean, label=\"Masked Mean\", color=\"red\")\n",
    "plt.fill_between(\n",
    "    range(full_masked_mean.shape[1]),\n",
    "    masked_mean - masked_std,\n",
    "    masked_mean + masked_std,\n",
    "    color=\"red\",\n",
    "    alpha=0.2,\n",
    "    label=\"Masked Std Dev\"\n",
    ")\n",
    "\n",
    "# Unmasked plot with mean and std\n",
    "plt.plot(range(full_unmasked_mean.shape[1]), unmasked_mean, label=\"Unmasked Mean\", color=\"blue\")\n",
    "plt.fill_between(\n",
    "    range(full_unmasked_mean.shape[1]),\n",
    "    unmasked_mean - unmasked_std,\n",
    "    unmasked_mean + unmasked_std,\n",
    "    color=\"blue\",\n",
    "    alpha=0.2,\n",
    "    label=\"Unmasked Std Dev\"\n",
    ")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([0, 3])\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Normalized Gradient Score\")\n",
    "plt.title(\"Mean of Tokens Gradient Score in a sentence per Iterations (over many trials)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "full_masked_maxs_np = np.array(full_masked_maxs)\n",
    "full_unmasked_maxs_np = np.array(full_unmasked_maxs)\n",
    "\n",
    "# Compute mean and standard deviation over trials (axis 0)\n",
    "masked_mean = np.mean(full_masked_maxs_np, axis=0)\n",
    "masked_std = np.std(full_masked_maxs_np, axis=0)\n",
    "\n",
    "unmasked_mean = np.mean(full_unmasked_maxs_np, axis=0)\n",
    "unmasked_std = np.std(full_unmasked_maxs_np, axis=0)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Masked plot with mean and std\n",
    "plt.plot(range(full_masked_maxs_np.shape[1]), masked_mean, label=\"Masked Mean\", color=\"red\")\n",
    "plt.fill_between(\n",
    "    range(full_masked_maxs_np.shape[1]),\n",
    "    masked_mean - masked_std,\n",
    "    masked_mean + masked_std,\n",
    "    color=\"red\",\n",
    "    alpha=0.2,\n",
    "    label=\"Masked Std Dev\"\n",
    ")\n",
    "\n",
    "# Unmasked plot with mean and std\n",
    "plt.plot(range(full_unmasked_maxs_np.shape[1]), unmasked_mean, label=\"Unmasked Mean\", color=\"blue\")\n",
    "plt.fill_between(\n",
    "    range(full_unmasked_maxs_np.shape[1]),\n",
    "    unmasked_mean - unmasked_std,\n",
    "    unmasked_mean + unmasked_std,\n",
    "    color=\"blue\",\n",
    "    alpha=0.2,\n",
    "    label=\"Unmasked Std Dev\"\n",
    ")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([0, 12])\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Normalized Gradient Score\")\n",
    "plt.title(\"Absolute Max of Tokens Gradient Score in a sentence per Iterations (over many trials)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "all_gradients = np.concatenate(\n",
    "    [np.array(grad.cpu()).reshape(-1, grad.shape[-1]) for grad in gradients], axis=0\n",
    ")  # Shape: [total_elements, embedding_dim]\n",
    "\n",
    "global_mean = np.mean(all_gradients, axis=0, keepdims=True)  # Shape: [1, embedding_dim]\n",
    "global_std = np.std(all_gradients, axis=0, keepdims=True) + 1e-8  # Shape: [1, embedding_dim]\n",
    "\n",
    "emb_score = []\n",
    "\n",
    "for it in range(len(gradients)):\n",
    "    grad = np.array(gradients[it].cpu())  # Convert to numpy for easier manipulation\n",
    "\n",
    "    # Normalize gradients globally\n",
    "    grad_norm = (grad - global_mean) / global_std  # Shape: [batch_size, token_length, embedding_dim]\n",
    "\n",
    "    # Mean over embeddings for each token across the batch\n",
    "    grad_mean_per_token = np.mean(abs(grad_norm), axis=(2))  # Shape: [batch_size,token_length]\n",
    "\n",
    "    emb_score.append(grad_mean_per_token)\n",
    "\n",
    "emb_score = np.array(emb_score)\n",
    "\n",
    "emb_score_mean_iter = np.mean(emb_score, axis=(0))  # Shape: [batch,token_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Example data\n",
    "tokens = sample_tokens[:20]\n",
    "gradients = emb_score_mean_iter[:20]\n",
    "\n",
    "# Replace this with your model's tokenizer decode function\n",
    "def decode_tokens(tokens):\n",
    "    \n",
    "    return [model.to_string(token) for token in tokens]\n",
    "\n",
    "# Decode tokens into words\n",
    "decoded_samples = [decode_tokens(sample) for sample in tokens]\n",
    "\n",
    "# Plotting\n",
    "num_samples = len(decoded_samples)\n",
    "fig, axes = plt.subplots(num_samples, 1, figsize=(10, 5 * num_samples), constrained_layout=True)\n",
    "\n",
    "if num_samples == 1:\n",
    "    axes = [axes]  # Ensure axes is always a list for consistent indexing\n",
    "\n",
    "for i, (words, grads) in enumerate(zip(decoded_samples, gradients)):\n",
    "    ax = axes[i]\n",
    "    ax.bar(range(len(words)), grads, color='skyblue')\n",
    "    ax.set_xticks(range(len(words)))\n",
    "    ax.set_xticklabels(words, rotation=45, ha='right', fontsize=12)\n",
    "    ax.set_title(f'Average gradient score over iterations', fontsize=16)\n",
    "    ax.set_ylabel('Gradient Score', fontsize=14)\n",
    "    ax.set_xlabel('Tokens', fontsize=14)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    ax.set_ylim([0, 2])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
