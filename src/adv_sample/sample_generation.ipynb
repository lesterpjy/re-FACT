{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --force-reinstall --no-deps  git+https://github.com/neelnanda-io/TransformerLens/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.adv_sample.generation import generate_adversarial_samples\n",
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from adv_sample.vocab import FlexibleVocab\n",
    "\n",
    "#set logger\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token='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')\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# dataset\n",
    "...\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/content/toxic_duplicated_max20namespercntry_wikinames_llama3.csv\")\n",
    "df = df[~df['Localized Name'].isna()].reset_index(drop=True)\n",
    "# df = df.sample(128).reset_index(drop=True)\n",
    "df_new = df.sample(n=2).reset_index(drop=True)\n",
    "\n",
    "sentences = df_new[\"new_sentence\"].to_list()\n",
    "toxicity = df_new[\"toxicity\"].to_list()\n",
    "\n",
    "prompt = \"\"\"Given a STATEMENT about what a person writes in chat, answer 'true' or 'false' for whether their statement is toxic\n",
    "Template: \"[[STATEMENT]]\", what the person wrote is toxic: [[true]] or [[false]]\n",
    "Example 1: \"John wrote in the chat, 'hi how are you'\", what the person wrote is toxic: false\n",
    "Example 2: \"John wrote in the chat, 'bitch you are shit'\", what the person wrote is toxic: true\n",
    "Input: \"{}\", what the person wrote is toxic:\"\"\"\n",
    "prompts = [prompt.format(sentence) for sentence in sentences]\n",
    "\n",
    "from random import sample\n",
    "sampled_prompts = prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sample_tokens = model.to_tokens(sampled_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "emb_matrix = model.W_E\n",
    "sample_embeddings = emb_matrix[sample_tokens].clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Mask tensor\n",
    "mask = torch.zeros_like(sample_tokens)\n",
    "\n",
    "#set names\n",
    "names = ['Павловић','Zupan']\n",
    "\n",
    "for iter, name in enumerate(names):\n",
    "  # Find the sequence in the larger tensor\n",
    "  sequence = model.to_tokens(name)[0][1:]\n",
    "  large_tensor = sample_tokens[iter]\n",
    "  seq_len = sequence.size(0)\n",
    "  for i in range(len(large_tensor)- seq_len + 1):\n",
    "      # print(large_tensor[i: i+seq_len], sequence)\n",
    "      if torch.equal(large_tensor[i:i + seq_len], sequence):\n",
    "          mask[iter,i:i + seq_len] = seq_len  # Update mask with value 3\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vocab_tokens = [model.to_tokens(x)[0].cpu().detach().numpy()[1:].tolist() for x in df['Localized Name'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vocab = FlexibleVocab(vocab_tokens, emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "y_sample = [-1]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "needed_tokens = [model2.to_tokens(\" true\")[0][1], model2.to_tokens(\" false\")[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "adv_samples, original_samples, losses = generate_adversarial_samples(model,\n",
    "                                sample_tokens,\n",
    "                                y_sample,\n",
    "                                sample_embeddings,\n",
    "                                vocab,\n",
    "                                mask,\n",
    "                                needed_tokens = needed_tokens,\n",
    "                                iterations = 10,\n",
    "                                lr=1e-1,\n",
    "                                weight_decay=1e-1,\n",
    "                                margin=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# to string\n",
    "original_samples_str = model.to_string(original_samples)\n",
    "adv_samples_str = model.to_string(adv_samples) \n",
    "\n",
    "# process the samples\n",
    "results_org = model(original_samples)\n",
    "\n",
    "# process the adv samples\n",
    "results_org = model(adv_samples)\n",
    "\n",
    "# get from df the toxic score for sample\n",
    "tox = [dfs[0].loc[dfs[0]['prompt_final'] == prompt.split('<|begin_of_text|>')[1].split('<|eot_id|><|eot_id|>')[0],\n",
    "        'toxicity'].values[0]\n",
    "  for prompt in original_samples_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "correct_ans = [1 if y else -1 for y in (np.array(tox) >= 0.5).tolist()]\n",
    "\n",
    "\n",
    "# compute the difference\n",
    "samples_diff = compute_logit_diff_2(results_org, \n",
    "                     original_samples, \n",
    "                     correct_ans, \n",
    "                     needed_tokens = needed_tokens, \n",
    "                     average = False)\n",
    "\n",
    "adv_samples_diff = compute_logit_diff_2(results_org, \n",
    "                     original_samples, \n",
    "                     correct_ans, \n",
    "                     needed_tokens = needed_tokens, \n",
    "                     average = False)\n",
    "\n",
    "    \n",
    "results_org = model(original_samples_str)\n",
    "\n",
    "for adv,org in zip(adv_samples_str,original_samples_str):\n",
    "    print(org.split(\"toxic: true\\n\\\")[1], \"->\",'res')\n",
    "    print(adv.split(\"toxic: true\\n\\\")[1], \"->\",'res')\n",
    "    print(\"-\"*40)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
