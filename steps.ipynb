{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from string import ascii_uppercase\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from sentence_transformers.util import semantic_search, dot_score, normalize_embeddings\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with open(f\"{data_path}/acronyms.txt\", \"r\") as f:\n",
    "   prompts, acronyms = list(zip(*[line.split(\", \") for line in f.read().splitlines()]))\n",
    "\n",
    "# take a subset of the dataset (we do this because VRAM limitations)\n",
    "n_samples = 300\n",
    "# giga-cursed way of sampling from the dataset\n",
    "prompts, acronyms = list(map(list, zip(*random.choices(list(zip(prompts, acronyms)), k=n_samples))))\n",
    "\n",
    "tokens = model.to_tokens(prompts)\n",
    "# ground truth: third letter of the acronym (tokens)\n",
    "y = model.to_tokens([x[letter] for x in acronyms], prepend_bos=False).squeeze()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(tokens)[:, indices_logits[letter]].argmax(dim=-1)\n",
    "# discard already misclassified samples\n",
    "tokens = tokens[y_pred == y]\n",
    "y = y[y_pred == y]\n",
    "# reupdate n_samples\n",
    "n_samples = tokens.shape[0]\n",
    "\n",
    "y_idx = y.cpu().apply_(token_to_idx.get).cuda()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
